{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ad43bfac",
   "metadata": {},
   "source": [
    "@notebook{feature_store-querying.ipynb,\n",
    "    title: Using feature store for feature querying using pandas like interface for query and join,\n",
    "    summary: Feature store quickstart guide to perform feature querying using pandas like interface for query and join.,\n",
    "    developed_on: pyspark32_p38_cpu_feature_store_v1,\n",
    "    keywords: feature store, querying,\n",
    "    license: Universal Permissive License v 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31964aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade Oracle ADS to pick up the latest preview version to maintain compatibility with Oracle Cloud Infrastructure.\n",
    "\n",
    "!odsc conda install --uri https://objectstorage.us-ashburn-1.oraclecloud.com/p/qnzzHQPGQYghdyH206yDk25MZH1FaMGdNNhKUl74BhRsW4muvFyGViKIqpxgnxI3/n/ociodscdev/b/ads_conda_pack_builds/o/PySpark_3/teamcity_20230512_084146_38972446/f227145b7ee5fc1c73a69ebaa671b81e/PySpark_3.2_and_Feature_Store.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d752b32e",
   "metadata": {},
   "source": [
    "Oracle Data Science service sample notebook.\n",
    "\n",
    "Copyright (c) 2022 Oracle, Inc. All rights reserved. Licensed under the [Universal Permissive License v 1.0](https://oss.oracle.com/licenses/upl).\n",
    "\n",
    "***\n",
    "\n",
    "# <font color=\"red\">Feature store handling querying operations</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Oracle Cloud Infrastructure Data Science Service.</font></p>\n",
    "\n",
    "---\n",
    "# Overview:\n",
    "---\n",
    "Managing many datasets, data-sources and transformations for machine learning is complex and costly. Poorly cleaned data, data issues, bugs in transformations, data drift and training serving skew all leads to increased model development time and worse model performance. Here, feature store is well positioned to solve many of the problems since it provides a centralised way to transform and access data for training and serving time and helps defines a standardised pipeline for ingestion of data and querying of data. This notebook demonstrates how to use feature store within a long lasting [Oracle Cloud Infrastructure Data Flow](https://docs.oracle.com/en-us/iaas/data-flow/using/home.htm) cluster.\n",
    "\n",
    "Compatible conda pack: [PySpark 3.2 and Feature store](https://docs.oracle.com/iaas/data-science/using/conda-pyspark-fam.htm) for CPU on Python 3.8\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href=\"#concepts\">1. Introduction</a>\n",
    "- <a href='#pre-requisites'>1. Pre-requisites</a>\n",
    "    - <a href='#policies'>2.1 Policies</a>\n",
    "    - <a href='#prerequisites_authentication'>2.2 Authentication</a>\n",
    "    - <a href='#prerequisites_variables'>2.3 Variables</a>\n",
    "- <a href='#featurestore_querying'>3. Feature store querying</a>\n",
    "    - <a href='#load_featuregroup'>3.1. Load feature groups</a>\n",
    "    - <a href='#explore_featuregroup'>3.2. Explore feature groups</a>\n",
    "    - <a href='#select_subset_featuregroup'>3.3. Select subset of features</a>\n",
    "    - <a href='#explore_featuregroup'>3.4. Explore feature groups</a>\n",
    "    - <a href='#filter_featuregroup'>3.5. Filter feature groups</a>\n",
    "    - <a href='#create_dataset'>3.6. Create dataset from multiple or one feature group</a>\n",
    "    - <a href='#query_dataset'>3.7 Query dataset</a>\n",
    "- <a href='#ref'>4. References</a>\n",
    "\n",
    "---\n",
    "\n",
    "**Important:**\n",
    "\n",
    "Placeholder text for required values are surrounded by angle brackets that must be removed when adding the indicated content. For example, when adding a database name to `database_name = \"<database_name>\"` would become `database_name = \"production\"`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4278249",
   "metadata": {},
   "source": [
    "<a id=\"concepts\"></a>\n",
    "# 1. Introduction\n",
    "\n",
    "Oracle feature store is a stack based solution that is deployed in the customer enclave using OCI resource manager. Customer can stand up the service with infrastructure in their own tenancy. The service consists of API which are deployed in customer tenancy using resource manager.\n",
    "\n",
    "The following are some key terms that will help you understand OCI Data Science Feature Store:\n",
    "\n",
    "\n",
    "* **Feature Vector**: Set of feature values for any one primary/identifier key. Eg.  All/subset of  features of customer id '2536' can be called as one feature vector.\n",
    "* **Feature**: A feature is an individual measurable property or characteristic of a phenomenon being observed.\n",
    "* **Entity**: An entity is a group of semantically related features.\n",
    "* **Datasource**: Features are engineered from raw data stored in various data sources (e.g. object storage, Oracle Database, Oracle MySQL, etc).\n",
    "* **Feature Group** - A feature group is an object that represents a logical group of time-series feature data as it is found in a datasource.\n",
    "* **Dataset**: Datasets are created from features stored in the feature store service and are used to train models and to perform online model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a288166",
   "metadata": {},
   "source": [
    "<a id='pre-requisites'></a>\n",
    "# 2. Pre-requisites\n",
    "\n",
    "Data Flow Sessions are accessible through the following conda environment:\n",
    "\n",
    "* **PySpark 3.2, Feature store 1.0 and Data Flow 1.0 (pyspark32_p38_cpu_feature_store_v1)**\n",
    "\n",
    "The [Data Catalog Hive Metastore](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm) provides schema definitions for objects in structured and unstructured data assets. The Metastore is the central metadata repository to understand tables backed by files on object storage. You can customize `pyspark32_p38_cpu_feature_store_v1`, publish it, and use it as a runtime environment for a Data Flow session cluster. The metastore id of hive metastore is tied to feature store construct of feature store service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad609733",
   "metadata": {},
   "source": [
    "<a id='policies'></a>\n",
    "### 2.1. Policies\n",
    "This section covers the creation of dynamic groups and policies needed to use the service.\n",
    "\n",
    "* [Data Flow Policies](https://docs.oracle.com/iaas/data-flow/using/policies.htm/)\n",
    "* [Data Catalog Metastore Required Policies](https://docs.oracle.com/en-us/iaas/data-catalog/using/metastore.htm)\n",
    "* [Getting Started with Data Flow](https://docs.oracle.com/iaas/data-flow/using/dfs_getting_started.htm)\n",
    "* [About Data Science Policies](https://docs.oracle.com/iaas/data-science/using/policies.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b64a2",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites_authentication\"></a>\n",
    "### 2.2. Authentication\n",
    "The [Oracle Accelerated Data Science SDK (ADS)](https://docs.oracle.com/iaas/tools/ads-sdk/latest/index.html) controls the authentication mechanism with the notebook cluster.<br>\n",
    "To setup authentication use the ```ads.set_auth(\"resource_principal\")``` or ```ads.set_auth(\"api_key\")```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d656d7f0",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "ads.set_auth(auth=\"api_key\", client_kwargs={\"service_endpoint\": \"http://localhost:21000/20230101\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f3b60",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites_variables\"></a>\n",
    "### 2.3. Variables\n",
    "To run this notebook, you must provide some information about your tenancy configuration. To create and run a feature store, you must specify a `<compartment_id>` and bucket `<metastore_id>` for offline feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a35bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "compartment_id = \"ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\"\n",
    "metastore_id = \"ocid1.datacatalogmetastore.oc1.iad.amaaaaaabiudgxyap7tizm4gscwz7amu7dixz7ml3mtesqzzwwg3urvvdgua\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae71fd5",
   "metadata": {},
   "source": [
    "<a id=\"featurestore_querying\"></a>\n",
    "# 3. Feature group querying\n",
    "By default the **PySpark 3.2, Feature store and Data Flow** conda environment includes pre-installed [great-expectations](https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/validation.html) and [deeque](https://github.com/awslabs/deequ) libraries. The joining functionality is heavily inspired by the APIs used by Pandas to merge, join or filter DataFrames. The APIs allow you to specify which features to select from which feature group, how to join them and which features to use in join conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a590c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:64: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/pandas/__init__.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  LooseVersion(pyarrow.__version__) >= LooseVersion(\"2.0.0\")\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/pandas/groupby.py:49: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pd.__version__) >= LooseVersion(\"1.3.0\"):\n",
      "\n",
      "ERROR:logger:Please set env variable SPARK_VERSION\n",
      "INFO:logger:Using deequ: com.amazon.deequ:deequ:1.2.2-spark-3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ads.feature_store.feature_store import FeatureStore\n",
    "from ads.feature_store.feature_group import FeatureGroup\n",
    "from ads.feature_store.dataset import Dataset\n",
    "from ads.feature_store.common.enums import DatasetIngestionMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14c3cf",
   "metadata": {},
   "source": [
    "<a id=\"data_exploration\"></a>\n",
    "### 3.1. Exploration of data in feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2d36c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/var/folders/pg/zfq6crsd7kdd3qw8txc6c1h00000gn/T/ipykernel_92740/4280186390.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights_df = pd.read_csv(\"~/Downloads/flights.csv\")\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRLINE</th>\n",
       "      <th>FLIGHT_NUMBER</th>\n",
       "      <th>TAIL_NUMBER</th>\n",
       "      <th>ORIGIN_AIRPORT</th>\n",
       "      <th>DESTINATION_AIRPORT</th>\n",
       "      <th>SCHEDULED_DEPARTURE</th>\n",
       "      <th>...</th>\n",
       "      <th>ARRIVAL_TIME</th>\n",
       "      <th>ARRIVAL_DELAY</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>CANCELLATION_REASON</th>\n",
       "      <th>AIR_SYSTEM_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>AIRLINE_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>98</td>\n",
       "      <td>N407AS</td>\n",
       "      <td>ANC</td>\n",
       "      <td>SEA</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>408.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>2336</td>\n",
       "      <td>N3KUAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>PBI</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>741.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "      <td>840</td>\n",
       "      <td>N171US</td>\n",
       "      <td>SFO</td>\n",
       "      <td>CLT</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>811.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AA</td>\n",
       "      <td>258</td>\n",
       "      <td>N3HYAA</td>\n",
       "      <td>LAX</td>\n",
       "      <td>MIA</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>756.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>AS</td>\n",
       "      <td>135</td>\n",
       "      <td>N527AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>ANC</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>259.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  MONTH  DAY  DAY_OF_WEEK AIRLINE  FLIGHT_NUMBER TAIL_NUMBER  \\\n",
       "0  2015      1    1            4      AS             98      N407AS   \n",
       "1  2015      1    1            4      AA           2336      N3KUAA   \n",
       "2  2015      1    1            4      US            840      N171US   \n",
       "3  2015      1    1            4      AA            258      N3HYAA   \n",
       "4  2015      1    1            4      AS            135      N527AS   \n",
       "\n",
       "  ORIGIN_AIRPORT DESTINATION_AIRPORT  SCHEDULED_DEPARTURE  ...  ARRIVAL_TIME  \\\n",
       "0            ANC                 SEA                    5  ...         408.0   \n",
       "1            LAX                 PBI                   10  ...         741.0   \n",
       "2            SFO                 CLT                   20  ...         811.0   \n",
       "3            LAX                 MIA                   20  ...         756.0   \n",
       "4            SEA                 ANC                   25  ...         259.0   \n",
       "\n",
       "   ARRIVAL_DELAY  DIVERTED  CANCELLED  CANCELLATION_REASON  AIR_SYSTEM_DELAY  \\\n",
       "0          -22.0         0          0                  NaN               NaN   \n",
       "1           -9.0         0          0                  NaN               NaN   \n",
       "2            5.0         0          0                  NaN               NaN   \n",
       "3           -9.0         0          0                  NaN               NaN   \n",
       "4          -21.0         0          0                  NaN               NaN   \n",
       "\n",
       "   SECURITY_DELAY  AIRLINE_DELAY  LATE_AIRCRAFT_DELAY  WEATHER_DELAY  \n",
       "0             NaN            NaN                  NaN            NaN  \n",
       "1             NaN            NaN                  NaN            NaN  \n",
       "2             NaN            NaN                  NaN            NaN  \n",
       "3             NaN            NaN                  NaN            NaN  \n",
       "4             NaN            NaN                  NaN            NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df = pd.read_csv(\"~/Downloads/flights.csv\")\n",
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8351d39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRPORT</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>Lehigh Valley International Airport</td>\n",
       "      <td>Allentown</td>\n",
       "      <td>PA</td>\n",
       "      <td>USA</td>\n",
       "      <td>40.65236</td>\n",
       "      <td>-75.44040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>Abilene Regional Airport</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>32.41132</td>\n",
       "      <td>-99.68190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>USA</td>\n",
       "      <td>35.04022</td>\n",
       "      <td>-106.60919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>Aberdeen Regional Airport</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>USA</td>\n",
       "      <td>45.44906</td>\n",
       "      <td>-98.42183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>Southwest Georgia Regional Airport</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.53552</td>\n",
       "      <td>-84.19447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                              AIRPORT         CITY STATE COUNTRY  \\\n",
       "0       ABE  Lehigh Valley International Airport    Allentown    PA     USA   \n",
       "1       ABI             Abilene Regional Airport      Abilene    TX     USA   \n",
       "2       ABQ    Albuquerque International Sunport  Albuquerque    NM     USA   \n",
       "3       ABR            Aberdeen Regional Airport     Aberdeen    SD     USA   \n",
       "4       ABY   Southwest Georgia Regional Airport       Albany    GA     USA   \n",
       "\n",
       "   LATITUDE  LONGITUDE  \n",
       "0  40.65236  -75.44040  \n",
       "1  32.41132  -99.68190  \n",
       "2  35.04022 -106.60919  \n",
       "3  45.44906  -98.42183  \n",
       "4  31.53552  -84.19447  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_df = pd.read_csv(\"~/Downloads/airports.csv\")\n",
    "airports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fc0d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IATA_CODE</th>\n",
       "      <th>AIRLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UA</td>\n",
       "      <td>United Air Lines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA</td>\n",
       "      <td>American Airlines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>US Airways Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F9</td>\n",
       "      <td>Frontier Airlines Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B6</td>\n",
       "      <td>JetBlue Airways</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IATA_CODE                 AIRLINE\n",
       "0        UA   United Air Lines Inc.\n",
       "1        AA  American Airlines Inc.\n",
       "2        US         US Airways Inc.\n",
       "3        F9  Frontier Airlines Inc.\n",
       "4        B6         JetBlue Airways"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airlines_df = pd.read_csv(\"~/Downloads/airlines.csv\")\n",
    "airlines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6813f7b7",
   "metadata": {},
   "source": [
    "<a id=\"load_featuregroup\"></a>\n",
    "### 3.2. Create feature store logical entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86e3ad",
   "metadata": {},
   "source": [
    "#### 3.2.1 Feature Store\n",
    "Feature store is the top level entity for feature store service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3911cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_resource = (\n",
    "    FeatureStore().\n",
    "    with_description(\"Data consisting of flights\").\n",
    "    with_compartment_id(compartment_id).\n",
    "    with_display_name(\"flights details\").\n",
    "    with_offline_config(metastore_id=metastore_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b979b7",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_store\"></a>\n",
    "##### Create Feature Store\n",
    "\n",
    "Call the ```.create()``` method of the Feature store instance to create a feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a80ffa54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: featurestore\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  description: Data consisting of flights\n",
       "  displayName: flights details\n",
       "  id: AF2A3D458EED647F3125C51FA0BFB30C\n",
       "  offlineConfig:\n",
       "    metastoreId: ocid1.datacatalogmetastore.oc1.iad.amaaaaaabiudgxyap7tizm4gscwz7amu7dixz7ml3mtesqzzwwg3urvvdgua\n",
       "type: featureStore"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store = feature_store_resource.create()\n",
    "feature_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83bd162",
   "metadata": {},
   "source": [
    "#### 3.2.2 Entity\n",
    "An entity is a group of semantically related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea2e9256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: entity\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  description: description for flight details\n",
       "  featureStoreId: AF2A3D458EED647F3125C51FA0BFB30C\n",
       "  id: A9CF6D8FA48651D156744107746AF78F\n",
       "  name: Flight details\n",
       "type: entity"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity = feature_store.create_entity(\n",
    "    display_name=\"Flight details\",\n",
    "    description=\"description for flight details\"\n",
    ")\n",
    "entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3501d",
   "metadata": {},
   "source": [
    "#### 3.2.3 Feature group\n",
    "A feature group is an object that represents a logical group of time-series feature data as it is found in a datasource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e5f23",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_group_flights\"></a>\n",
    "##### Flights Feature Group\n",
    "\n",
    "Create feature group for flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627b5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_flights = (\n",
    "    FeatureGroup()\n",
    "    .with_feature_store_id(feature_store.id)\n",
    "    .with_primary_keys([\"FLIGHT_NUMBER\"])\n",
    "    .with_name(\"flights_feature_group\")\n",
    "    .with_entity_id(entity.id)\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_schema_details_from_dataframe(flights_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b17133d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: A9CF6D8FA48651D156744107746AF78F\n",
       "  featureStoreId: AF2A3D458EED647F3125C51FA0BFB30C\n",
       "  id: D4CBEA0B8E8E23CB560750DB3022D89E\n",
       "  inputFeatureDetails:\n",
       "  - featureType: INTEGER\n",
       "    name: YEAR\n",
       "    orderNumber: 1\n",
       "  - featureType: INTEGER\n",
       "    name: MONTH\n",
       "    orderNumber: 2\n",
       "  - featureType: INTEGER\n",
       "    name: DAY\n",
       "    orderNumber: 3\n",
       "  - featureType: INTEGER\n",
       "    name: DAY_OF_WEEK\n",
       "    orderNumber: 4\n",
       "  - featureType: STRING\n",
       "    name: AIRLINE\n",
       "    orderNumber: 5\n",
       "  - featureType: INTEGER\n",
       "    name: FLIGHT_NUMBER\n",
       "    orderNumber: 6\n",
       "  - featureType: STRING\n",
       "    name: TAIL_NUMBER\n",
       "    orderNumber: 7\n",
       "  - featureType: STRING\n",
       "    name: ORIGIN_AIRPORT\n",
       "    orderNumber: 8\n",
       "  - featureType: STRING\n",
       "    name: DESTINATION_AIRPORT\n",
       "    orderNumber: 9\n",
       "  - featureType: INTEGER\n",
       "    name: SCHEDULED_DEPARTURE\n",
       "    orderNumber: 10\n",
       "  - featureType: FLOAT\n",
       "    name: DEPARTURE_TIME\n",
       "    orderNumber: 11\n",
       "  - featureType: FLOAT\n",
       "    name: DEPARTURE_DELAY\n",
       "    orderNumber: 12\n",
       "  - featureType: FLOAT\n",
       "    name: TAXI_OUT\n",
       "    orderNumber: 13\n",
       "  - featureType: FLOAT\n",
       "    name: WHEELS_OFF\n",
       "    orderNumber: 14\n",
       "  - featureType: FLOAT\n",
       "    name: SCHEDULED_TIME\n",
       "    orderNumber: 15\n",
       "  - featureType: FLOAT\n",
       "    name: ELAPSED_TIME\n",
       "    orderNumber: 16\n",
       "  - featureType: FLOAT\n",
       "    name: AIR_TIME\n",
       "    orderNumber: 17\n",
       "  - featureType: INTEGER\n",
       "    name: DISTANCE\n",
       "    orderNumber: 18\n",
       "  - featureType: FLOAT\n",
       "    name: WHEELS_ON\n",
       "    orderNumber: 19\n",
       "  - featureType: FLOAT\n",
       "    name: TAXI_IN\n",
       "    orderNumber: 20\n",
       "  - featureType: INTEGER\n",
       "    name: SCHEDULED_ARRIVAL\n",
       "    orderNumber: 21\n",
       "  - featureType: FLOAT\n",
       "    name: ARRIVAL_TIME\n",
       "    orderNumber: 22\n",
       "  - featureType: FLOAT\n",
       "    name: ARRIVAL_DELAY\n",
       "    orderNumber: 23\n",
       "  - featureType: INTEGER\n",
       "    name: DIVERTED\n",
       "    orderNumber: 24\n",
       "  - featureType: INTEGER\n",
       "    name: CANCELLED\n",
       "    orderNumber: 25\n",
       "  - featureType: STRING\n",
       "    name: CANCELLATION_REASON\n",
       "    orderNumber: 26\n",
       "  - featureType: FLOAT\n",
       "    name: AIR_SYSTEM_DELAY\n",
       "    orderNumber: 27\n",
       "  - featureType: FLOAT\n",
       "    name: SECURITY_DELAY\n",
       "    orderNumber: 28\n",
       "  - featureType: FLOAT\n",
       "    name: AIRLINE_DELAY\n",
       "    orderNumber: 29\n",
       "  - featureType: FLOAT\n",
       "    name: LATE_AIRCRAFT_DELAY\n",
       "    orderNumber: 30\n",
       "  - featureType: FLOAT\n",
       "    name: WEATHER_DELAY\n",
       "    orderNumber: 31\n",
       "  name: flights_feature_group\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: FLIGHT_NUMBER\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_flights.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972bcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"DEVELOPER_MODE\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28763e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/kshitizlohia/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/kshitizlohia/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e865a4e0-a3ac-4af8-814e-278888fbf345;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in central\n",
      "\tfound io.delta#delta-storage;2.3.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in local-m2-cache\n",
      ":: resolution report :: resolve 166ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.3.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e865a4e0-a3ac-4af8-814e-278888fbf345\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 15:25:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "\n",
      "WARNING:py.warnings:/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 76, in ingest_feature_definition\n",
      "    self._save_offline_dataframe(dataframe, feature_group, feature_group_job)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 183, in _save_offline_dataframe\n",
      "    raw_schema = self.spark_engine.convert_from_pandas_to_spark_dataframe(\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/engine/spark_engine.py\", line 154, in convert_from_pandas_to_spark_dataframe\n",
      "    return self.spark.createDataFrame(dataframe)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 891, in createDataFrame\n",
      "    return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py\", line 437, in createDataFrame\n",
      "    return self._create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 936, in _create_dataframe\n",
      "    rdd, struct = self._createFromLocal(map(prepare, data), schema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 631, in _createFromLocal\n",
      "    struct = self._inferSchemaFromList(data, names=schema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 517, in _inferSchemaFromList\n",
      "    schema = reduce(\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1383, in _merge_type\n",
      "    fields = [\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1385, in <listcomp>\n",
      "    f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()), name=new_name(f.name))\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1378, in _merge_type\n",
      "    raise TypeError(new_msg(\"Can not merge type %s and %s\" % (type(a), type(b))))\n",
      "TypeError: field CANCELLATION_REASON: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/pg/zfq6crsd7kdd3qw8txc6c1h00000gn/T/ipykernel_92740/1672186503.py\", line 1, in <module>\n",
      "    feature_group_flights.materialise(flights_df)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/feature_group.py\", line 813, in materialise\n",
      "    feature_group_execution_strategy.ingest_feature_definition(\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 78, in ingest_feature_definition\n",
      "    raise SparkExecutionException(e).with_traceback(e.__traceback__)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 76, in ingest_feature_definition\n",
      "    self._save_offline_dataframe(dataframe, feature_group, feature_group_job)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/spark/spark_execution.py\", line 183, in _save_offline_dataframe\n",
      "    raw_schema = self.spark_engine.convert_from_pandas_to_spark_dataframe(\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/execution_strategy/engine/spark_engine.py\", line 154, in convert_from_pandas_to_spark_dataframe\n",
      "    return self.spark.createDataFrame(dataframe)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 891, in createDataFrame\n",
      "    return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py\", line 437, in createDataFrame\n",
      "    return self._create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 936, in _create_dataframe\n",
      "    rdd, struct = self._createFromLocal(map(prepare, data), schema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 631, in _createFromLocal\n",
      "    struct = self._inferSchemaFromList(data, names=schema)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/session.py\", line 517, in _inferSchemaFromList\n",
      "    schema = reduce(\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1383, in _merge_type\n",
      "    fields = [\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1385, in <listcomp>\n",
      "    f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()), name=new_name(f.name))\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/sql/types.py\", line 1378, in _merge_type\n",
      "    raise TypeError(new_msg(\"Can not merge type %s and %s\" % (type(a), type(b))))\n",
      "ads.feature_store.execution_strategy.spark.spark_execution.SparkExecutionException: field CANCELLATION_REASON: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>\n",
      "SparkExecutionException: field CANCELLATION_REASON: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
     ]
    }
   ],
   "source": [
    "feature_group_flights.materialise(flights_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ef662",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_group_airport\"></a>\n",
    "##### Airport Feature Group\n",
    "\n",
    "Create feature group for airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c831661",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_airports = (\n",
    "    FeatureGroup()\n",
    "    .with_feature_store_id(feature_store.id)\n",
    "    .with_primary_keys([\"IATA_CODE\"])\n",
    "    .with_name(\"airport_feature_group\")\n",
    "    .with_entity_id(entity.id)\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_schema_details_from_dataframe(airports_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aafe022",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: A9CF6D8FA48651D156744107746AF78F\n",
       "  featureStoreId: AF2A3D458EED647F3125C51FA0BFB30C\n",
       "  id: FD1325A83A2068800CDE45E68D278025\n",
       "  inputFeatureDetails:\n",
       "  - featureType: STRING\n",
       "    name: IATA_CODE\n",
       "    orderNumber: 1\n",
       "  - featureType: STRING\n",
       "    name: AIRPORT\n",
       "    orderNumber: 2\n",
       "  - featureType: STRING\n",
       "    name: CITY\n",
       "    orderNumber: 3\n",
       "  - featureType: STRING\n",
       "    name: STATE\n",
       "    orderNumber: 4\n",
       "  - featureType: STRING\n",
       "    name: COUNTRY\n",
       "    orderNumber: 5\n",
       "  - featureType: FLOAT\n",
       "    name: LATITUDE\n",
       "    orderNumber: 6\n",
       "  - featureType: FLOAT\n",
       "    name: LONGITUDE\n",
       "    orderNumber: 7\n",
       "  name: airport_feature_group1\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: IATA_CODE\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airports.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d8f9a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 15:31:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/05/15 15:31:44 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/05/15 15:31:46 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 15:31:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 15:31:53 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `a9cf6d8fa48651d156744107746af78f`.`airport_feature_group1` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_group_airports.materialise(airports_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfd19d",
   "metadata": {},
   "source": [
    "<a id=\"create_feature_group_airlines\"></a>\n",
    "##### Airlines Feature Group\n",
    "\n",
    "Create feature group for airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9217e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_airlines = (\n",
    "    FeatureGroup()\n",
    "    .with_feature_store_id(feature_store.id)\n",
    "    .with_primary_keys([\"IATA_CODE\"])\n",
    "    .with_name(\"airlines_feature_group\")\n",
    "    .with_entity_id(entity.id)\n",
    "    .with_compartment_id(compartment_id)\n",
    "    .with_schema_details_from_dataframe(airlines_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b24a4920",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: FeatureGroup\n",
       "spec:\n",
       "  compartmentId: ocid1.tenancy.oc1..aaaaaaaa462hfhplpx652b32ix62xrdijppq2c7okwcqjlgrbknhgtj2kofa\n",
       "  entityId: A9CF6D8FA48651D156744107746AF78F\n",
       "  featureStoreId: AF2A3D458EED647F3125C51FA0BFB30C\n",
       "  id: 89A0CDE87CA616E4BE814A5F07631459\n",
       "  inputFeatureDetails:\n",
       "  - featureType: STRING\n",
       "    name: IATA_CODE\n",
       "    orderNumber: 1\n",
       "  - featureType: STRING\n",
       "    name: AIRLINE\n",
       "    orderNumber: 2\n",
       "  name: airlines_feature_group\n",
       "  primaryKeys:\n",
       "    items:\n",
       "    - name: IATA_CODE\n",
       "  statisticsConfig:\n",
       "    isEnabled: true\n",
       "type: featureGroup"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_airlines.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5936119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 15:32:40 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `a9cf6d8fa48651d156744107746af78f`.`airlines_feature_group` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "feature_group_airlines.materialise(airlines_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a74e07",
   "metadata": {},
   "source": [
    "<a id=\"explore_featuregroup\"></a>\n",
    "### 3.3. Explore feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96756df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR - Exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/IPython/core/formatters.py\", line 223, in catch_format_error\n",
      "    r = method(self, *args, **kwargs)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/IPython/core/formatters.py\", line 708, in __call__\n",
      "    printer.pretty(obj)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/IPython/lib/pretty.py\", line 410, in pretty\n",
      "    return _repr_pprint(obj, self, cycle)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/IPython/lib/pretty.py\", line 778, in _repr_pprint\n",
      "    output = repr(obj)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/jobs/builders/base.py\", line 135, in __repr__\n",
      "    return self.to_yaml()\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/jobs/serializer.py\", line 172, in to_yaml\n",
      "    yaml_string = yaml.dump(self.to_dict(), Dumper=dumper)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/ads/feature_store/query/query.py\", line 367, in to_dict\n",
      "    query = copy.deepcopy(self._spec)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 271, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 271, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 172, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 271, in _reconstruct\n",
      "    state = deepcopy(state, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 146, in deepcopy\n",
      "    y = copier(x, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\n",
      "    y[deepcopy(key, memo)] = deepcopy(value, memo)\n",
      "  File \"/usr/local/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py\", line 161, in deepcopy\n",
      "    rv = reductor(4)\n",
      "  File \"/Users/kshitizlohia/IdeaProjects/oracle/feature-store/advanced-ds/venv/lib/python3.10/site-packages/pyspark/context.py\", line 447, in __getnewargs__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n",
      "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "feature_group_airlines.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3be4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "+---------+--------------------+\n",
      "|       EV|Atlantic Southeas...|\n",
      "|       HA|Hawaiian Airlines...|\n",
      "|       UA|United Air Lines ...|\n",
      "|       WN|Southwest Airline...|\n",
      "|       DL|Delta Air Lines Inc.|\n",
      "|       MQ|American Eagle Ai...|\n",
      "|       VX|      Virgin America|\n",
      "|       OO|Skywest Airlines ...|\n",
      "|       AS|Alaska Airlines Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_group_airlines.select().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccebb2",
   "metadata": {},
   "source": [
    "<a id=\"select_subset_featuregroup\"></a>\n",
    "### 3.4. Select subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2704c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_flights.select(['YEAR', 'MONTH', 'DATE', 'FLIGHT_NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81003f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|IATA_CODE|             AIRLINE|\n",
      "+---------+--------------------+\n",
      "|       EV|Atlantic Southeas...|\n",
      "|       HA|Hawaiian Airlines...|\n",
      "|       UA|United Air Lines ...|\n",
      "|       WN|Southwest Airline...|\n",
      "|       DL|Delta Air Lines Inc.|\n",
      "|       MQ|American Eagle Ai...|\n",
      "|       VX|      Virgin America|\n",
      "|       OO|Skywest Airlines ...|\n",
      "|       AS|Alaska Airlines Inc.|\n",
      "|       F9|Frontier Airlines...|\n",
      "+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_group_airlines.select(['IATA_CODE', 'AIRLINE']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033bf7a",
   "metadata": {},
   "source": [
    "<a id=\"filter_featuregroup\"></a>\n",
    "### 3.5. Filter feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994dd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_airlines.filter(feature_group_airlines.CANCELLED == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0e9a5",
   "metadata": {},
   "source": [
    "<a id=\"join_featuregroup\"></a>\n",
    "### 3.6. Apply joins on feature group\n",
    "As in Pandas, if the feature has the same name on both feature groups, then you can use the `on=[]` paramter. If they have different names, then you can use the `left_on=[]` and `right_on=[]` paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = feature_group_flights.select()\\\n",
    "                .join(feature_group_airlines.select(), left_on=['ORIGIN_AIRPORT'], right_on=['IATA_CODE'])\\\n",
    "                .join(feature_group_airports.select(), left_on=['AIRLINE'], right_on=['IATA_CODE'])\n",
    "query.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd1e29",
   "metadata": {},
   "source": [
    "<a id=\"create_dataset\"></a>\n",
    "### 3.7 Create dataset\n",
    "A dataset is a collection of feature snapshots that are joined together to either train a model or perform model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eddc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\\\n",
    "    .with_description(\"Combined dataset for flights\")\\\n",
    "    .with_compartment_id(compartment_id)\\\n",
    "    .with_display_name(\"Flight dataset\")\\\n",
    "    .with_entity_id(entity.id)\\\n",
    "    .with_dataset_ingestion_mode(DatasetIngestionMode.SQL)\\\n",
    "    .with_feature_store_id(feature_store.id)\\\n",
    "    .with_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc762c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.materialise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5536e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e6936",
   "metadata": {},
   "source": [
    "<a id=\"query_dataset\"></a>\n",
    "### 3.8 Query dataset\n",
    "Feature store provides a DataFrame API to ingest data into the Feature Store. You can also retrieve feature data in a DataFrame, that can either be used directly to train models or materialized to file(s) for later use to train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = dataset_df.select([\"IATA_CODE\", \"ORIGIN_AIRPORT\"])\n",
    "query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35641534",
   "metadata": {},
   "source": [
    "<a id=\"sql_query\"></a>\n",
    "### 3.9 SQL query\n",
    "Feature store provides a way to query feature store using free flow query. User need to mention `entity id` as the database name and `feature group name` as the table name to query feature store. This functionality can be useful if you need to express more complex queries for your use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ef2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id = entity.id\n",
    "\n",
    "sql = (f\"SELECT flights_feature_group.* \"\n",
    "       f\"FROM {entity_id}.flights_feature_group \"\n",
    "       f\"JOIN {entity_id}.airport_feature_group \"\n",
    "       f\"ON {entity_id}.flights_feature_group.ORIGIN_AIRPORT={entity_id}.airport_feature_group.IATA_CODE \"\n",
    "       f\"JOIN {entity_id}.flights_feature_group.AIRLINE={entity_id}.airlines_feature_group.IATA_CODE \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467e9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store.sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6cc5bc",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# References\n",
    "\n",
    "- [ADS Library Documentation](https://accelerated-data-science.readthedocs.io/en/latest/index.html)\n",
    "- [Data Science YouTube Videos](https://www.youtube.com/playlist?list=PLKCk3OyNwIzv6CWMhvqSB_8MLJIZdO80L)\n",
    "- [OCI Data Science Documentation](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm)\n",
    "- [Oracle Data & AI Blog](https://blogs.oracle.com/datascience/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
