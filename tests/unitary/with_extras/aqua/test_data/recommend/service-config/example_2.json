{
  "shape": [
    "BM.GPU.A10.4",
    "VM.GPU.A10.2",
    "BM.GPU.H100.8",
    "BM.GPU.H200.8",
    "BM.GPU.A100-v2.8",
    "BM.GPU.B4.8",
    "BM.GPU4.8"
  ],
  "configuration": {
    "BM.GPU.H100.8": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4"
      },
      "multi_model_deployment": [
        {
          "gpu_count": 1,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 1"
          }
        },
        {
          "gpu_count": 2,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 2"
          }
        },
        {
          "gpu_count": 4,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 4"
          }
        }
      ]
    },
    "BM.GPU.H200.8": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4"
      },
      "multi_model_deployment": [
        {
          "gpu_count": 1,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 1"
          }
        },
        {
          "gpu_count": 2,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 2"
          }
        },
        {
          "gpu_count": 4,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 4"
          }
        }
      ]
    },
    "BM.GPU.A10.4": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 130000 --dtype bfloat16"
      },
      "env": {
        "VLLM": {
          "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
        }
      },
      "multi_model_deployment": [
        {
          "gpu_count": 1,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 2048 --quantization mxfp4 --tensor-parallel-size 1"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        },
        {
          "gpu_count": 2,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --quantization mxfp4 --tensor-parallel-size 2"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        }
      ]
    },
    "VM.GPU.A10.2": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 5 --max-model-len 8192 --dtype bfloat16"
      },
      "env": {
        "VLLM": {
          "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
        }
      },
      "multi_model_deployment": [
        {
          "gpu_count": 1,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 2048 --quantization mxfp4 --tensor-parallel-size 1"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        }
      ]
    },
    "BM.GPU.A100-v2.8": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --dtype bfloat16"
      },
      "env": {
        "VLLM": {
          "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
        }
      }
    },
    "BM.GPU.B4.8": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 130000 --dtype bfloat16"
      },
      "multi_model_deployment": [
        {
          "gpu_count": 2,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --dtype bfloat16 --tensor-parallel-size 2"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        },
        {
          "gpu_count": 4,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --dtype bfloat16 --tensor-parallel-size 4"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        }
      ]
    },
    
    "BM.GPU4.8": {
      "parameters": {
        "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 130000 --dtype bfloat16"
      },
      "env": {
        "VLLM": {
          "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
        }
      },
      "multi_model_deployment": [
        {
          "gpu_count": 2,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 32 --max-model-len 130000 --dtype bfloat16 --tensor-parallel-size 2"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        },
        {
          "gpu_count": 4,
          "parameters": {
            "VLLM_PARAMS": "--trust-remote-code --gpu-memory-utilization 0.90 --max-num-seqs 64 --max-model-len 130000 --dtype bfloat16 --tensor-parallel-size 4"
          },
          "env": {
            "VLLM": {
              "VLLM_ATTENTION_BACKEND": "TRITON_ATTN_VLLM_V1"
            }
          }
        }
      ]
    }
  }
}