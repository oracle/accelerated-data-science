# This YAML specification was auto generated by the `ads opctl init` command.
# The more details about the jobs YAML specification can be found in the ADS documentation:
# https://accelerated-data-science.readthedocs.io/en/latest/user_guide/apachespark/dataflow.html


kind: job
spec:
  infrastructure:
    kind: infrastructure
    spec:
      compartmentId: ocid1.compartment.oc1..<unique_id>
      driverShape: VM.Standard.E2.4
      executorShape: VM.Standard.E2.4
      language: PYTHON
      logsBucketUri: oci://bucket@namespace
      numExecutors: '1'
      scriptBucket: oci://bucket@namespace/prefix
      sparkVersion: 3.2.1
    type: dataFlow
  name: '{Job name. For MLflow and Operator will be auto generated}'
  runtime:
    kind: runtime
    spec:
      args: []
      conda:
        type: published
        uri: oci://bucket@namespace/conda_environments/test/conda/slug
      condaAuthType: resource_principal
      configuration:
        spark.driverEnv.env_key: env_value
      freeformTags:
        tag_name: tag_value
      overwrite: true
      scriptBucket: '{The object storage bucket to save a script. Example: oci://<bucket_name>@<tenancy>/<prefix>}'
      scriptPathURI: '{Path to the executable script. For MLflow and Operator will be auto generated}'
    type: dataFlowNotebook
